{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12440152,"sourceType":"datasetVersion","datasetId":7847265}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-09-12T15:44:56.840845Z","iopub.status.busy":"2025-09-12T15:44:56.840601Z","iopub.status.idle":"2025-09-12T15:44:57.675934Z","shell.execute_reply":"2025-09-12T15:44:57.674808Z","shell.execute_reply.started":"2025-09-12T15:44:56.840819Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndef print_introduction():\n    \"\"\"Prints an introduction and frames the problem for the teen phone addiction dataset.\"\"\"\n\n    print(\"--- üì± The Teen Phone Addiction Study üß† ---\")\n    print(\"\\n\")\n    print(\"Welcome to a data-driven exploration of a modern-day challenge: teen phone addiction.\")\n    print(\"This project aims to analyze a dataset containing information about teens'\")\n    print(\"phone usage, social habits, academic performance, and mental well-being.\")\n    print(\"By understanding the relationships between these factors, we can gain insights\")\n    print(\"into the potential impacts of excessive screen time.\")\n\n    print(\"\\n## The Problem Statement üéØ\")\n    print(\"Our primary goal is to build a machine learning model that can predict\")\n    print(\"a teen's **Addiction Level** based on various features like their daily\")\n    print(\"phone usage hours, sleep patterns, and social interactions.\")\n    print(\"\\nTo achieve this, we will follow a standard machine learning pipeline:\")\n    print(\"1. Data Cleaning & Preprocessing üßπ\")\n    print(\"2. Exploratory Data Analysis (EDA) üìä\")\n    print(\"3. Feature Engineering & Selection üõ†Ô∏è\")\n    print(\"4. Model Training & Evaluation ü§ñ\")\n    print(\"5. Model Deployment (Conceptual) üöÄ\")\n    \n    print(\"\\nOur success will be measured by how accurately our model can predict\")\n    print(\"the addiction level, providing a valuable tool for understanding and\")\n    print(\"addressing this societal issue.\")\n    print(\"\\n--- Let's get started! üöÄ ---\")\n\nif __name__ == \"__main__\":\n    print_introduction()","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:51:30.874793Z","iopub.status.busy":"2025-09-12T15:51:30.874480Z","iopub.status.idle":"2025-09-12T15:51:30.882378Z","shell.execute_reply":"2025-09-12T15:51:30.881489Z","shell.execute_reply.started":"2025-09-12T15:51:30.874774Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\n\ndef load_teen_phone_addiction_data():\n    \"\"\"\n    Load the teen phone addiction dataset with fallback to sample data generation.\n    First tries to load from Kaggle input, then generates sample data if not found.\n    \"\"\"\n    # Try to load from Kaggle input first\n    kaggle_path = '/kaggle/input/teen-phone-addiction/teen_phone_addiction_dataset.csv'\n    \n    try:\n        print(\"üîÑ Attempting to load dataset from Kaggle input...\")\n        df = pd.read_csv(kaggle_path)\n        print(f\"‚úÖ Successfully loaded dataset from Kaggle input: {df.shape}\")\n        return df\n    except FileNotFoundError:\n        print(\"‚ö†Ô∏è Kaggle input file not found. Generating sample dataset...\")\n        return generate_sample_dataset()\n\ndef generate_sample_dataset(n_samples=1000):\n    \"\"\"\n    Generate a sample teen phone addiction dataset matching the expected schema.\n    \"\"\"\n    print(\"üîÑ Generating sample dataset...\")\n    \n    # Set random seed for reproducibility\n    np.random.seed(42)\n    random.seed(42)\n    \n    # Sample data for categorical variables\n    genders = ['Male', 'Female', 'Other']\n    locations = ['Hansonfort', 'Theodorefort', 'Lindseystad', 'West Anthony', 'Port Lindsaystad', \n                'East Angelachester', 'North Jeffrey', 'Jenniferport', 'Leebury', 'Prestonview']\n    phone_purposes = ['Browsing', 'Social Media', 'Gaming', 'Education', 'Other']\n    \n    # Generate sample data\n    data = {\n        'ID': range(1, n_samples + 1),\n        'Name': [f\"Sample User {i}\" for i in range(1, n_samples + 1)],\n        'Age': np.random.randint(13, 20, n_samples),\n        'Gender': np.random.choice(genders, n_samples),\n        'Location': np.random.choice(locations, n_samples),\n        'School_Grade': np.random.choice([f\"{i}th\" for i in range(7, 13)], n_samples),\n        'Daily_Usage_Hours': np.random.uniform(1.0, 10.0, n_samples).round(1),\n        'Sleep_Hours': np.random.uniform(3.0, 10.0, n_samples).round(1),\n        'Academic_Performance': np.random.randint(50, 101, n_samples),\n        'Social_Interactions': np.random.randint(0, 11, n_samples),\n        'Exercise_Hours': np.random.uniform(0.0, 4.0, n_samples).round(1),\n        'Anxiety_Level': np.random.randint(1, 11, n_samples),\n        'Depression_Level': np.random.randint(1, 11, n_samples),\n        'Self_Esteem': np.random.randint(1, 11, n_samples),\n        'Parental_Control': np.random.randint(0, 2, n_samples),\n        'Screen_Time_Before_Bed': np.random.uniform(0.1, 3.0, n_samples).round(1),\n        'Phone_Checks_Per_Day': np.random.randint(20, 150, n_samples),\n        'Apps_Used_Daily': np.random.randint(5, 21, n_samples),\n        'Time_on_Social_Media': np.random.uniform(0.0, 6.0, n_samples).round(1),\n        'Time_on_Gaming': np.random.uniform(0.0, 5.0, n_samples).round(1),\n        'Time_on_Education': np.random.uniform(0.0, 3.0, n_samples).round(1),\n        'Phone_Usage_Purpose': np.random.choice(phone_purposes, n_samples),\n        'Family_Communication': np.random.randint(1, 11, n_samples),\n        'Weekend_Usage_Hours': np.random.uniform(2.0, 12.0, n_samples).round(1),\n    }\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    # Generate Addiction_Level based on other features (more realistic)\n    addiction_factors = (\n        df['Daily_Usage_Hours'] * 0.3 +\n        df['Time_on_Social_Media'] * 0.2 +\n        df['Phone_Checks_Per_Day'] * 0.01 +\n        df['Screen_Time_Before_Bed'] * 0.1 +\n        (10 - df['Self_Esteem']) * 0.1 +\n        df['Anxiety_Level'] * 0.05 +\n        df['Depression_Level'] * 0.05 +\n        np.random.normal(0, 0.5, n_samples)\n    )\n    \n    # Normalize to 0-10 range\n    df['Addiction_Level'] = np.clip(addiction_factors, 0, 10).round(1)\n    \n    print(f\"‚úÖ Generated sample dataset with {n_samples} samples: {df.shape}\")\n    print(\"üìä Sample data preview:\")\n    print(df.head())\n    \n    return df\n\n# Load the dataset\ndf = load_teen_phone_addiction_data()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset\n# Note: The file could not be accessed in this environment, but this is the standard way to load it.\nfile_name = '/kaggle/input/teen-phone-addiction/teen_phone_addiction_dataset.csv'\n\ntry:\n    df = pd.read_csv(file_name)\n\n    # 1. Handle Missing Values: Check for nulls and handle as needed\n    print(\"Checking for missing values:\")\n    print(df.isnull().sum())\n    # Based on initial inspection, this dataset has no missing values.\n    # If it did, you might fill them with: df.fillna(df.mean(), inplace=True)\n\n    # 2. Handle Duplicate Rows\n    num_duplicates = df.duplicated().sum()\n    if num_duplicates > 0:\n        print(f\"\\nRemoving {num_duplicates} duplicate rows.\")\n        df.drop_duplicates(inplace=True)\n    else:\n        print(\"\\nNo duplicate rows found.\")\n\n    # 3. Correcting Data Types: Convert 'School_Grade' from string to integer\n    # The 'th' suffix needs to be removed from a column like 'School_Grade'.\n    df['School_Grade'] = df['School_Grade'].astype(str).str.replace(r'th|st|nd|rd', '', regex=True).astype(int)\n    print(\"\\nCorrected 'School_Grade' data type:\")\n    print(df['School_Grade'].head())\n\n    # 4. Handle Categorical Features using One-Hot Encoding\n    # This converts text labels into a numerical format.\n    categorical_cols = ['Gender', 'Location', 'School_Grade', 'Phone_Usage_Purpose']\n    df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n    print(\"\\nDataFrame after one-hot encoding:\")\n    df_encoded.info()\n\n    # Save the cleaned and preprocessed data to a new CSV file\n    cleaned_file_name = 'cleaned_teen_phone_addiction_dataset.csv'\n    df_encoded.to_csv(cleaned_file_name, index=False)\n    print(f\"\\nCleaned and preprocessed data saved to '{cleaned_file_name}'\")\n\nexcept FileNotFoundError:\n    print(f\"Error: The file '{file_name}' was not found. Please ensure the file is in the same directory as the script.\")","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:51:47.503603Z","iopub.status.busy":"2025-09-12T15:51:47.503282Z","iopub.status.idle":"2025-09-12T15:51:50.295710Z","shell.execute_reply":"2025-09-12T15:51:50.294929Z","shell.execute_reply.started":"2025-09-12T15:51:47.503579Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display the loaded dataset\ndf","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:44:57.678601Z","iopub.status.busy":"2025-09-12T15:44:57.678231Z","iopub.status.idle":"2025-09-12T15:44:57.761408Z","shell.execute_reply":"2025-09-12T15:44:57.760220Z","shell.execute_reply.started":"2025-09-12T15:44:57.678578Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.execute_input":"2025-09-12T16:23:27.132971Z","iopub.status.busy":"2025-09-12T16:23:27.132638Z","iopub.status.idle":"2025-09-12T16:23:27.145228Z","shell.execute_reply":"2025-09-12T16:23:27.144181Z","shell.execute_reply.started":"2025-09-12T16:23:27.132947Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.execute_input":"2025-09-12T16:23:16.826476Z","iopub.status.busy":"2025-09-12T16:23:16.826149Z","iopub.status.idle":"2025-09-12T16:23:16.883505Z","shell.execute_reply":"2025-09-12T16:23:16.882743Z","shell.execute_reply.started":"2025-09-12T16:23:16.826453Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Check the column names to ensure 'Daily Usage Hours' and 'Gender' exist\nprint(df.columns)\n\n# Bar plot: Average Daily Usage Hours by Gender\nplt.figure(figsize=(8,5))\nsns.barplot(data=df, x='Gender', y='Daily_Usage_Hours', ci='sd', palette='pastel')\nplt.title('Average Daily Phone Usage Hours by Gender')\nplt.ylabel('Daily_Usage_Hours')\nplt.xlabel('Gender')\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:44:57.835922Z","iopub.status.busy":"2025-09-12T15:44:57.835607Z","iopub.status.idle":"2025-09-12T15:44:59.666360Z","shell.execute_reply":"2025-09-12T15:44:59.665363Z","shell.execute_reply.started":"2025-09-12T15:44:57.835893Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Replace 'Daily Usage Hours' with the correct column name if needed\nplt.figure(figsize=(8,5))\nsns.histplot(df['Daily_Usage_Hours'], kde=True, bins=20, color='skyblue')\nplt.title('Distribution of Daily Screen Time')\nplt.xlabel('Daily Usage Hours')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:44:59.667555Z","iopub.status.busy":"2025-09-12T15:44:59.667251Z","iopub.status.idle":"2025-09-12T15:45:00.074197Z","shell.execute_reply":"2025-09-12T15:45:00.073165Z","shell.execute_reply.started":"2025-09-12T15:44:59.667526Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Check exact column name for social media time\nprint(df.columns)\n\n# Replace 'Time on Social Media (hours)' with the exact column name if different\nplt.figure(figsize=(10,6))\nsns.barplot(data=df, x='Age', y='Time_on_Social_Media', ci='sd', palette='muted')\nplt.title('Average Time on Social Media by Age')\nplt.xlabel('Age')\nplt.ylabel('Time on Social Media (hours)')\nplt.xticks(rotation=45)\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:45:00.076027Z","iopub.status.busy":"2025-09-12T15:45:00.075609Z","iopub.status.idle":"2025-09-12T15:45:00.318052Z","shell.execute_reply":"2025-09-12T15:45:00.317325Z","shell.execute_reply.started":"2025-09-12T15:45:00.075995Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Check column names\nprint(df.columns)\n\n# Group by Age and calculate average Self_Esteem\nage_self_esteem = df.groupby('Age')['Self_Esteem'].mean()\n\n# Plot as pie chart\nplt.figure(figsize=(8,8))\nage_self_esteem.plot.pie(autopct='%1.1f%%', startangle=140, cmap='Set3')\nplt.title('Average Self-Esteem by Age')\nplt.ylabel('')  # Hide y-label for cleaner look\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:45:00.319284Z","iopub.status.busy":"2025-09-12T15:45:00.318997Z","iopub.status.idle":"2025-09-12T15:45:00.583260Z","shell.execute_reply":"2025-09-12T15:45:00.582137Z","shell.execute_reply.started":"2025-09-12T15:45:00.319259Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Grouping by Daily_Usage_Hours to get average Addiction_Level per usage hour\ngrouped_df = df.groupby('Daily_Usage_Hours')['Addiction_Level'].mean().reset_index()\n\n# Create the line plot\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=grouped_df, x='Daily_Usage_Hours', y='Addiction_Level', marker='o', color='blue')\nplt.title('Line Plot: Daily Usage Hours vs Addiction Level')\nplt.xlabel('Daily Usage Hours')\nplt.ylabel('Average Addiction Level')\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:45:00.584685Z","iopub.status.busy":"2025-09-12T15:45:00.584453Z","iopub.status.idle":"2025-09-12T15:45:00.861202Z","shell.execute_reply":"2025-09-12T15:45:00.860160Z","shell.execute_reply.started":"2025-09-12T15:45:00.584667Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Group by Age to get average Depression_Level per age\ngrouped_df = df.groupby('Age')['Depression_Level'].mean().reset_index()\n\n# Create the line plot\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=grouped_df, x='Age', y='Depression_Level', marker='o', color='green')\nplt.title('Line Plot: Age vs Depression Level')\nplt.xlabel('Age')\nplt.ylabel('Average Depression Level')\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:45:00.862533Z","iopub.status.busy":"2025-09-12T15:45:00.862217Z","iopub.status.idle":"2025-09-12T15:45:01.124747Z","shell.execute_reply":"2025-09-12T15:45:01.123755Z","shell.execute_reply.started":"2025-09-12T15:45:00.862505Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Group by Age to get average Family_Communication per age\ngrouped_df = df.groupby('Age')['Family_Communication'].mean().reset_index()\n\n# Create the line plot\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=grouped_df, x='Age', y='Family_Communication', marker='o', color='purple')\nplt.title('Line Plot: Age vs Family Communication')\nplt.xlabel('Age')\nplt.ylabel('Average Family Communication Score')\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:45:01.126440Z","iopub.status.busy":"2025-09-12T15:45:01.125893Z","iopub.status.idle":"2025-09-12T15:45:01.393909Z","shell.execute_reply":"2025-09-12T15:45:01.392922Z","shell.execute_reply.started":"2025-09-12T15:45:01.126413Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Filter for a specific Daily Usage Hour, e.g., 5 hours\nusage_hour = 5\nfiltered_df = df[df['Daily_Usage_Hours'] == usage_hour]\n\n# Count Phone Usage Purpose distribution\npurpose_counts = filtered_df['Phone_Usage_Purpose'].value_counts()\n\n# Create pie chart\nplt.figure(figsize=(8, 8))\nplt.pie(purpose_counts, labels=purpose_counts.index, autopct='%1.1f%%', startangle=140)\nplt.title(f'Phone Usage Purpose Distribution for {usage_hour} Daily Usage Hours')\nplt.axis('equal')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:45:01.395037Z","iopub.status.busy":"2025-09-12T15:45:01.394754Z","iopub.status.idle":"2025-09-12T15:45:01.588236Z","shell.execute_reply":"2025-09-12T15:45:01.587213Z","shell.execute_reply.started":"2025-09-12T15:45:01.395017Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Group by Exercise_Hours to get average Self_Esteem\ngrouped_df = df.groupby('Exercise_Hours')['Self_Esteem'].mean().reset_index()\n\n# Create the line plot\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=grouped_df, x='Exercise_Hours', y='Self_Esteem', marker='o', color='teal')\nplt.title('Line Plot: Exercise Hours vs Self Esteem')\nplt.xlabel('Exercise Hours')\nplt.ylabel('Average Self Esteem Score')\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:45:01.589544Z","iopub.status.busy":"2025-09-12T15:45:01.589222Z","iopub.status.idle":"2025-09-12T15:45:01.868538Z","shell.execute_reply":"2025-09-12T15:45:01.867715Z","shell.execute_reply.started":"2025-09-12T15:45:01.589516Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Group by Exercise_Hours to get average Addiction_Level\ngrouped_df = df.groupby('Exercise_Hours')['Addiction_Level'].mean().reset_index()\n\n# Line plot\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=grouped_df, x='Exercise_Hours', y='Addiction_Level', marker='o', color='darkblue')\nplt.title('Line Plot: Exercise Hours vs Addiction Level')\nplt.xlabel('Exercise Hours')\nplt.ylabel('Average Addiction Level')\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:45:01.869833Z","iopub.status.busy":"2025-09-12T15:45:01.869537Z","iopub.status.idle":"2025-09-12T15:45:02.173419Z","shell.execute_reply":"2025-09-12T15:45:02.172580Z","shell.execute_reply.started":"2025-09-12T15:45:01.869808Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Group by Age and calculate average time spent on each activity\ngrouped_df = df.groupby('Age')[['Time_on_Social_Media', 'Time_on_Gaming', 'Time_on_Education']].mean().reset_index()\n\n# Create line plot\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=grouped_df, x='Age', y='Time_on_Social_Media', marker='o', label='Social Media')\nsns.lineplot(data=grouped_df, x='Age', y='Time_on_Gaming', marker='o', label='Gaming')\nsns.lineplot(data=grouped_df, x='Age', y='Time_on_Education', marker='o', label='Education')\n\nplt.title('Line Plot: Age vs Time Spent on Activities')\nplt.xlabel('Age')\nplt.ylabel('Average Time (hours)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:45:02.174820Z","iopub.status.busy":"2025-09-12T15:45:02.174556Z","iopub.status.idle":"2025-09-12T15:45:02.515316Z","shell.execute_reply":"2025-09-12T15:45:02.514386Z","shell.execute_reply.started":"2025-09-12T15:45:02.174801Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Group by Time_on_Social_Media to get average Academic_Performance\ngrouped_df = df.groupby('Time_on_Social_Media')['Academic_Performance'].mean().reset_index()\n\n# Create the line plot\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=grouped_df, x='Time_on_Social_Media', y='Academic_Performance', marker='o', color='darkgreen')\nplt.title('Line Plot: Time on Social Media vs Academic Performance')\nplt.xlabel('Time on Social Media (hours)')\nplt.ylabel('Average Academic Performance')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:45:02.516416Z","iopub.status.busy":"2025-09-12T15:45:02.516187Z","iopub.status.idle":"2025-09-12T15:45:02.770573Z","shell.execute_reply":"2025-09-12T15:45:02.769602Z","shell.execute_reply.started":"2025-09-12T15:45:02.516399Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport io\n\ndef generate_boxplots_all_numeric(file_content):\n    \"\"\"\n    Loads data from a string, generates and saves boxplots for all numerical features,\n    excluding specified columns.\n\n    Args:\n        file_content (str): The content of the CSV file as a string.\n    \"\"\"\n    try:\n        # Load the dataset from the string content\n        df = pd.read_csv(io.StringIO(file_content))\n\n        # Define columns to exclude from plotting\n        exclude_cols = ['ID', 'Name', 'Location']\n        \n        # Identify all numeric columns to plot\n        numeric_cols = [\n            col for col in df.select_dtypes(include=['int64', 'float64']).columns\n            if col not in exclude_cols\n        ]\n        \n        # Determine the number of subplots needed\n        num_plots = len(numeric_cols)\n        num_rows = (num_plots + 3) // 4  # Calculate number of rows, 4 plots per row\n        num_cols = 4\n\n        # Set up the plotting environment\n        fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, 5 * num_rows))\n        axes = axes.flatten()\n\n        # Generate a boxplot for each numerical column\n        for i, col in enumerate(numeric_cols):\n            axes[i].boxplot(df[col].dropna())\n            axes[i].set_title(col.replace('_', ' ').replace('Time on', 'Time on\\n').replace('Checks Per', 'Checks Per\\n'))\n            axes[i].set_ylabel('Value')\n\n        # Hide any unused subplots\n        for j in range(num_plots, len(axes)):\n            fig.delaxes(axes[j])\n\n        plt.suptitle('Boxplots for All Numerical Features', fontsize=16)\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        \n        # Save the figure to a file\n        plot_file = 'all_numeric_boxplots.png'\n        plt.savefig(plot_file)\n        print(f\"Boxplots saved to {plot_file}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# The file content fetched in the previous step is available as a string.\nfile_content = \"\"\"ID,Name,Age,Gender,Location,School_Grade,Daily_Usage_Hours,Sleep_Hours,Academic_Performance,Social_Interactions,Exercise_Hours,Anxiety_Level,Depression_Level,Self_Esteem,Parental_Control,Screen_Time_Before_Bed,Phone_Checks_Per_Day,Apps_Used_Daily,Time_on_Social_Media,Time_on_Gaming,Time_on_Education,Phone_Usage_Purpose,Family_Communication,Weekend_Usage_Hours,Addiction_Level\n1,Shannon Francis,13,Female,Hansonfort,9th,4.0,6.1,78,5,0.1,10,3,8,0,1.4,86,19,3.6,1.7,1.2,Browsing,4,8.7,10.0\n2,Scott Rodriguez,17,Female,Theodorefort,7th,5.5,6.5,70,5,0.0,3,7,3,0,0.9,96,9,1.1,4.0,1.8,Browsing,2,5.3,10.0\n3,Adrian Knox,13,Other,Lindseystad,11th,5.8,5.5,93,8,0.8,2,3,10,0,0.5,137,8,0.3,1.5,0.4,Education,6,5.7,9.2\n4,Brittany Hamilton,18,Female,West Anthony,12th,3.1,3.9,78,8,1.6,9,10,3,0,1.4,128,7,3.1,1.6,0.8,Social Media,8,3.0,9.8\n5,Steven Smith,14,Other,Port Lindsaystad,9th,2.5,6.7,56,4,1.1,1,5,1,0,1.0,96,20,2.6,0.9,1.1,Gaming,10,3.7,8.6\n6,Mary Adams,13,Female,East Angelachester,10th,3.9,6.3,89,3,0.7,7,1,3,0,1.1,135,8,3.8,0.0,1.4,Social Media,7,6.0,8.8\n7,Hailey Moses,16,Male,North Jeffrey,11th,6.3,6.7,89,3,0.9,6,7,9,0,0.8,129,9,1.8,2.7,1.0,Education,7,7.8,10.0\n8,Veronica Marshall,13,Other,Jenniferport,10th,5.1,6.1,70,2,2.2,5,6,8,0,1.0,34,7,2.3,1.6,0.5,Browsing,9,8.0,8.0\n9,Edward Avila,13,Male,Leebury,8th,3.0,9.1,79,0,1.8,1,7,6,0,0.9,70,13,2.7,2.2,1.3,Education,10,9.1,7.3\n10,James Carter,18,Other,Prestonview,11th,3.9,5.8,89,8,1.1,9,1,9,0,0.9,121,13,2.7,0.4,1.0,Other,9,2.9,9.1\n11,Bobby Sparks,18,Female,Cherylburgh,12th,4.9,7.0,74,5,0.6,4,3,2,0,0.6,84,20,3.1,0.6,0.8,Other,6,3.5,10.0\n12,Ruben Walters,14,Male,Lake Mary,11th,1.6,5.9,70,10,1.5,10,8,8,0,0.9,147,18,1.9,2.7,1.0,Browsing,7,3.2,9.8\n13,Sarah Nguyen,19,Female,South Heatherton,10th,7.4,5.8,70,4,0.6,4,9,3,0,1.1,59,19,4.3,0.4,0.4,Gaming,8,9.1,10.0\n14,Melanie Phillips,17,Female,Port Angelaburgh,9th,8.7,5.5,95,6,1.9,2,8,2,1,0.7,76,8,4.2,1.3,1.5,Education,5,4.6,10.0\n15,Edwin Lambert,15,Other,New Emilyberg,12th,6.4,8.5,79,3,0.0,2,8,9,1,0.5,50,14,5.0,1.2,0.0,Social Media,5,5.6,10.0\n16,Gregory Hughes,19,Other,Port Charles,11th,4.4,6.6,97,6,1.6,8,10,4,1,1.2,134,17,2.2,1.5,1.8,Education,4,6.8,10.0\n17,Matthew Webb,15,Female,Lake Chase,8th,5.4,7.8,87,4,2.3,6,6,9,0,0.9,36,20,2.6,1.3,1.1,Gaming,9,1.4,10.0\n18,John Cooper,15,Other,Paulstad,10th,3.0,6.1,80,5,1.4,1,4,2,1,1.2,95,16,3.0,1.9,0.2,Other,7,7.5,10.0\n19,Amy Greene,19,Female,Port Nicholasberg,9th,7.9,8.5,96,6,0.1,5,2,4,1,1.0,58,9,2.2,1.9,1.3,Education,5,3.5,10.0\n20,Kathryn Perez,18,Male,South Thomas,12th,5.4,5.9,69,4,1.3,2,7,8,0,0.6,83,19,1.6,1.0,0.4,Browsing,7,6.1,10.0\n21,Michael Williams,15,Female,Margaretmouth,12th,3.0,5.1,83,4,1.1,1,9,4,1,1.5,58,6,1.7,1.7,2.0,Social Media,8,6.7,6.2\n22,Patricia Johnson,16,Female,North Scott,12th,2.7,6.3,92,3,1.4,2,9,3,1,1.7,112,17,2.1,2.6,0.9,Gaming,10,5.7,9.9\n23,Jennifer Davis,14,Other,Port Angelabury,11th,2.7,5.1,60,9,0.0,4,6,2,0,1.7,117,16,2.5,1.3,0.2,Education,5,4.8,9.5\n24,Desiree Brady,19,Male,East Amanda,9th,6.9,3.6,79,5,1.0,5,5,10,1,0.8,32,7,2.0,1.3,1.2,Education,7,6.6,10.0\n25,Mark Crawford,19,Other,Port Alan,12th,4.6,7.2,85,7,1.3,7,6,4,0,1.1,71,5,1.8,0.8,1.1,Social Media,8,6.2,6.0\n26,Samantha Sanchez,16,Male,Port Bianca,11th,5.5,5.9,78,5,1.2,1,7,3,0,0.7,44,14,0.9,1.3,1.3,Gaming,3,3.9,8.4\n27,Kelly Giles,19,Male,West Kimberlyfurt,11th,6.2,4.5,65,3,1.3,5,7,1,0,1.1,115,10,2.1,1.6,1.0,Social Media,7,6.6,10.0\n28,Jeffrey Crawford,19,Male,Shawburgh,8th,4.9,9.3,50,4,0.0,7,4,1,0,0.3,117,5,3.5,4.0,0.9,Education,3,7.0,10.0\n29,Tammy Gallegos,17,Female,West Cynthia,10th,6.6,8.1,77,7,1.0,6,1,3,1,1.5,147,14,2.0,1.7,1.6,Education,8,9.2,10.0\n30,Jared Bridges,19,Other,Jessicaberg,11th,8.6,5.2,63,0,1.1,6,4,8,0,1.3,72,15,0.2,1.8,0.7,Browsing,9,6.6,10.0\n31,Bailey Flores,15,Other,Lake Martha,12th,4.1,4.5,59,8,0.8,10,1,8,0,0.6,32,10,2.2,1.0,0.5,Education,9,9.3,7.7\n32,Jennifer Hudson,19,Male,Vickifurt,10th,4.2,6.9,86,8,1.7,5,1,9,1,1.0,100,15,2.6,2.2,0.9,Browsing,4,6.9,10.0\n33,Kathleen Friedman,19,Male,Port Ryan,10th,8.0,5.5,87,7,2.2,1,8,5,0,1.0,26,16,3.3,1.3,2.6,Gaming,6,5.2,10.0\n34,Cynthia Fisher,16,Male,Lake Jamiebury,9th,3.3,5.3,50,4,0.0,7,1,6,0,0.3,141,14,0.7,3.2,0.0,Gaming,5,5.1,10.0\n35,Amy Carroll,16,Male,South Williamfort,12th,4.7,6.6,59,6,1.5,10,4,7,1,0.6,123,18,1.0,1.1,1.2,Social Media,2,0.3,9.6\n36,Monica Roberts,15,Male,West Kristinaview,8th,3.9,6.3,64,9,1.3,6,7,8,1,0.9,135,11,1.2,1.5,1.5,Social Media,7,2.2,8.4\n37,Laura Steele,13,Female,Burketown,8th,4.4,6.8,62,8,1.4,2,1,10,0,1.7,66,7,0.9,0.0,0.0,Browsing,1,6.9,4.6\n38,Lauren Matthews,13,Male,Christineton,9th,8.3,6.2,65,9,1.3,5,6,8,1,1.3,144,5,2.3,0.0,1.5,Other,1,2.9,10.0\n39,Marcus Stephens,16,Other,Schmitthaven,9th,7.4,6.9,67,0,1.7,6,6,8,0,1.6,117,16,1.6,0.7,0.7,Social Media,8,4.6,10.0\n40,Cameron Hansen,16,Female,North Christina,12th,1.7,6.8,68,9,0.8,5,9,1,1,0.7,107,18,2.4,0.6,0.0,Education,6,6.2,7.0\n41,Richard Choi,13,Other,Jasonmouth,8th,6.2,8.3,77,1,0.4,7,2,4,0,1.1,141,5,3.8,1.4,0.9,Browsing,8,6.2,10.0\n42,Nicholas Gomez,19,Other,Roberttown,12th,5.2,7.2,96,5,0.1,4,10,3,0,0.7,119,16,3.0,1.7,0.6,Gaming,3,6.1,10.0\n43,Jessica Smith,15,Other,Andreville,11th,3.3,5.3,89,7,0.0,9,6,9,0,0.3,77,18,1.1,1.0,0.0,Gaming,9,10.4,7.9\n44,Brittany Myers,13,Male,Nguyenfort,11th,6.9,6.2,89,4,1.1,3,2,7,1,0.3,140,13,3.0,1.9,0.7,Gaming,9,4.1,10.0\n45,Alicia White,13,Male,Millerton,9th,6.6,5.0,88,8,2.1,10,7,10,0,2.0,56,20,2.1,1.4,0.4,Education,9,6.2,10.0\n46,Brandi King,18,Female,Evanschester,8th,1.0,6.2,60,5,1.4,6,3,10,0,0.4,25,16,3.1,0.0,2.0,Gaming,4,5.8,4.7\n47,Kenneth Travis,17,Male,Castilloside,8th,6.7,4.7,67,3,1.3,10,1,3,0,0.8,129,6,2.8,0.8,1.7,Gaming,7,9.9,10.0\n48,James Sanchez,19,Other,Adamsside,11th,7.1,7.1,67,2,0.4,1,1,4,0,2.6,119,16,2.8,2.4,0.0,Gaming,4,5.3,10.0\n49,Tyrone Cabrera,18,Male,New Ericview,7th,4.7,6.5,69,6,1.5,10,3,1,0,1.3,119,5,3.4,1.0,0.0,Gaming,10,4.7,9.2\n50,Curtis Castro,14,Male,East Amber,11th,8.1,7.2,91,6,1.4,8,7,8,1,0.3,120,13,1.4,2.2,1.9,Education,9,6.0,10.0\n51,Steven Chavez,14,Male,Yuport,9th,4.3,5.9,75,4,2.5,5,1,8,1,2.2,105,17,1.9,1.4,1.4,Other,1,3.1,10.0\n52,Cody Young,17,Male,Savagetown,11th,3.9,8.4,97,3,0.5,1,7,8,0,1.2,66,13,4.3,1.6,1.0,Education,6,4.9,9.5\n53,Victoria Clark,13,Other,Collinston,9th,9.0,7.3,79,6,1.1,1,1,7,1,0.8,127,17,3.1,1.5,0.0,Education,1,6.9,10.0\n54,Peggy Bond,15,Other,East Laurieview,8th,4.5,7.8,93,5,1.0,7,7,2,1,1.3,83,17,2.6,1.2,2.4,Education,8,6.0,9.5\n55,Candace Anderson,14,Other,Kimberlyburgh,11th,4.4,4.1,56,9,1.7,7,7,1,1,0.8,129,16,4.9,1.4,0.6,Other,10,6.1,10.0\n56,Tammy Payne,19,Other,South Thomas,12th,5.9,8.5,87,2,0.0,4,10,9,0,1.5,48,6,3.0,1.5,0.7,Social Media,2,6.5,8.3\n57,Mrs. Lauren Ayala,17,Other,Port Dustin,10th,3.3,6.1,78,2,3.6,2,9,9,1,1.2,141,17,1.9,1.0,0.2,Education,8,5.1,9.4\n58,Edward Jackson,15,Female,Davidberg,12th,2.9,5.5,71,1,1.1,9,2,1,1,0.9,117,10,2.0,1.6,1.3,Gaming,7,4.6,8.1\n59,Kathleen Hoffman,17,Male,Walkerburgh,11th,3.4,4.7,79,10,0.9,8,4,5,0,0.6,123,6,3.0,1.7,0.7,Other,5,7.6,9.4\n60,Kelly Carrillo,17,Female,West Tracey,7th,6.3,7.4,77,8,1.6,4,4,10,0,1.4,110,8,3.7,1.5,1.9,Gaming,1,3.1,10.0\n61,Derek York,19,Male,Shafferville,10th,3.7,7.9,89,9,0.7,8,7,7,0,0.3,92,7,0.9,2.1,0.9,Education,7,2.0,6.0\n62,Ashley Velasquez,18,Male,Johnsonstad,7th,4.7,6.9,77,7,0.9,6,10,1,0,1.0,105,12,0.7,1.3,0.0,Education,8,8.3,7.8\n63,Sara Williams,17,Other,Traceymouth,8th,6.7,7.9,54,8,1.3,7,10,7,0,0.2,84,9,2.2,1.3,1.4,Education,9,7.9,9.7\n64,Linda Fletcher,18,Female,Lake Samanthabury,12th,5.3,7.7,63,1,2.2,10,6,4,1,0.7,135,6,1.7,2.4,1.6,Other,9,5.7,9.4\n65,Jaclyn Salazar,17,Male,Alejandraburgh,12th,8.7,5.2,87,5,0.0,4,8,2,1,1.2,146,10,4.0,0.6,0.4,Gaming,6,5.9,10.0\n66,Stephanie Fletcher,17,Other,Port Jonathanside,9th,3.9,8.0,80,3,1.5,4,5,10,0,0.3,41,11,2.6,0.0,1.1,Social Media,2,5.7,5.5\n67,Zachary Parks,16,Female,Bradyton,10th,7.4,6.6,100,3,1.1,2,3,5,1,1.3,90,19,2.1,0.8,0.6,Other,9,9.3,10.0\n68,Chelsey Dillon,18,Female,Franciston,11th,6.8,10.0,98,7,1.7,7,8,3,1,1.7,72,16,2.4,1.0,2.0,Social Media,10,7.6,9.8\n69,Elijah Lewis,18,Female,Candacemouth,8th,7.0,8.0,70,5,1.9,8,5,7,1,0.7,133,9,1.6,0.6,0.4,Gaming,5,1.8,9.7\n70,Joseph Franklin,16,Male,West Lisaview,8th,5.8,7.2,88,5,1.0,9,9,2,0,0.7,30,15,2.6,0.7,0.5,Browsing,10,9.7,9.1\n71,Christine Hansen,17,Other,Jamesbury,12th,4.1,5.5,67,2,0.2,1,7,4,1,0.5,120,16,4.3,2.1,2.0,Browsing,8,11.0,10.0\n72,Kristen Holmes,19,Other,Hickschester,12th,4.9,4.4,86,5,1.8,7,6,7,1,0.9,90,15,2.5,1.2,0.5,Other,2,4.2,10.0\n73,Michelle Ellis,18,Male,West Erikashire,12th,4.4,6.6,95,3,1.1,8,1,3,1,0.6,91,15,2.2,1.6,0.8,Education,1,3.9,9.7\n74,Dr. David Howard,19,Male,South Taylortown,8th,6.3,3.0,58,8,1.1,5,2,8,0,0.6,48,6,3.9,1.1,0.1,Other,6,8.5,10.0\n75,Michael Tucker,15,Male,New Gregstad,9th,5.8,6.9,84,8,2.3,3,9,3,1,0.5,106,7,3.1,1.2,1.5,Social Media,6,5.5,10.0\n76,Brenda Lee,13,Female,Morganshire,8th,6.5,7.2,58,10,0.3,9,6,5,0,1.1,74,6,2.6,2.0,1.1,Gaming,9,8.7,10.0\n77,Jennifer Levine,17,Female,West Michael,9th,5.8,7.0,76,8,1.7,5,5,5,1,1.7,116,19,3.0,2.2,2.2,Education,9,3.3,10.0\n78,Joshua Moore,19,Other,Gonzalezland,7th,5.1,7.0,64,4,1.5,10,6,7,0,1.7,140,5,0.5,1.7,0.9,Gaming,2,7.7,7.6\n79,Marcus Gibson,13,Other,East Thomas,8th,5.2,6.0,87,0,0.8,1,5,8,1,1.6,79,11,4.2,0.0,1.2,Gaming,6,3.7,10.0\n80,Mackenzie Wright,13,Male,Antoniohaven,11th,4.8,8.6,78,8,0.0,3,3,10,1,1.0,51,9,3.0,2.4,1.6,Social Media,7,7.1,8.7\n81,Christopher Smith,19,Male,Lake Susanport,11th,4.3,4.3,91,6,0.0,9,8,5,0,0.9,98,7,2.2,0.6,0.3,Social Media,8,5.2,8.3\n82,Andrew Kim,19,Female,North Cristina,7th,7.6,6.4,88,4,0.0,6,4,8,0,1.8,77,15,4.6,0.0,0.0,Gaming,7,7.1,10.0\n83,Rebecca Reynolds,17,Female,Lake Adrianberg,7th,5.5,6.9,65,3,0.4,6,9,9,1,0.4,82,6,1.6,1.1,0.2,Browsing,4,6.2,7.6\n84,Catherine Bradshaw,15,Other,Sarahfurt,10th,4.8,6.3,76,8,1.6,2,6,7,1,1.1,136,11,4.8,1.3,1.2,Social Media,8,5.3,10.0\n85,Tammy Carter,19,Other,Marktown,9th,7.4,6.3,75,6,1.5,9,7,2,1,0.3,85,9,2.9,2.9,0.0,Social Media,1,7.6,10.0\n86,Maria Ochoa,16,Male,South Tonyton,12th,3.8,5.1,85,1,0.0,8,7,8,0,0.6,104,10,1.8,0.5,1.3,Social Media,9,9.3,7.6\n87,James Herrera,13,Other,Lake Nancy,10th,7.3,5.6,84,1,0.2,4,1,8,0,1.2,22,11,2.2,2.0,0.9,Other,1,4.1,10.0\n88,Brian Coleman,19,Female,Kevinport,8th,0.7,4.6,64,6,1.3,10,1,9,0,0.4,146,10,2.2,0.2,1.3,Other,3,5.3,5.7\n89,Gregory Martinez,14,Male,South Adamside,11th,3.0,6.6,82,0,0.8,1,3,3,0,0.6,84,6,3.7,2.3,1.0,Other,9,6.7,8.6\n90,Richard Nichols,19,Other,East Andrewport,12th,4.0,7.4,83,3,0.3,10,4,4,0,0.6,55,18,4.1,0.8,0.0,Gaming,3,7.4,9.9\n91,Steven Tanner,16,Other,Craigstad,12th,8.1,6.8,59,1,0.0,7,3,1,0,0.6,129,7,0.7,2.5,1.5,Other,1,5.1,10.0\n92,Michael Farmer,17,Male,Guerrafort,12th,5.2,6.6,56,10,0.9,6,8,4,0,0.5,98,16,2.4,1.4,1.0,Other,5,8.1,10.0\n93,Keith Ward,18,Other,Parsonsfurt,7th,1.7,6.2,52,8,1.3,2,2,7,0,1.3,76,17,1.1,1.9,0.7,Social Media,7,6.5,6.5\n94,Aaron Parker,15,Male,Port Maryborough,8th,6.8,7.4,100,9,1.1,6,7,7,0,1.9,123,11,4.2,1.6,0.7,Social Media,5,6.8,10.0\n95,Wesley Miller,18,Female,Sherryton,9th,6.4,10.0,89,0,1.2,9,9,8,0,0.5,128,9,4.1,3.2,1.4,Education,2,4.6,10.0\n96,Tiffany Santiago,17,Male,Wilsonton,9th,7.2,7.4,63,7,0.1,2,3,8,0,0.7,112,11,3.8,0.5,0.5,Social Media,4,2.3,10.0\n97,Lindsey Sloan,18,Female,Lake Adamfort,10th,3.3,5.7,93,5,1.7,9,6,7,1,0.1,72,17,3.6,1.2,0.3,Other,10,6.6,10.0\n98,Kristin Martin,16,Male,East Patrickburgh,8th,4.7,8.5,98,3,0.2,4,5,8,0,0.1,20,16,2.8,1.3,0.3,Social Media,8,6.2,8.2\n99,Valerie Blevins,18,Female,East Lynn,9th,2.8,7.0,93,5,0.8,4,5,2,0,1.5,42,6,2.8,1.0,0.6,Social Media,10,8.2,5.1\n100,Morgan Mills,16,Female,East Derrick,8th,6.2,8.7,53,1,0.1,7,6,10,1,0.5,24,12,1.6,2.5,1.0,Other,7,6.7,8.8\n101,Christian Perkins,17,Male,East Wendymouth,9th,6.7,8.5,91,4,1.4,8,7,2,0,0.2,49,9,2.1,3.5,1.5,Browsing,8,5.7,10.0\n102,John Evans,14,Other,Port Pamelatown,8th,2.2,5.1,69,7,1.0,6,3,8,1,1.3,24,16,2.6,2.2,1.1,Other,9,7.6,8.1\n103,Ashley Chang,13,Female,Harrisbury,11th,4.1,6.3,57,3,2.0,5,3,7,0,0.4,78,13,1.7,2.2,0.3,Gaming,3,5.8,9.0\n104,Julia Ross,13,Male,Taraland,11th,5.3,5.9,85,9,0.6,6,1,2,1,0.9,98,14,2.3,2.6,0.9,Education,8,8.9,10.0\n105,Ashley Foster,18,Female,South Shelleyfort,8th,4.7,3.4,71,1,1.8,7,4,7,1,1.5,87,9,0.1,0.0,0.0,Education,7,6.7,6.6\n106,Katherine Hensley,19,Other,East Angelaland,10th,4.5,7.4,67,9,1.1,10,1,2,1,0.8,68,11,2.6,1.0,0.9,Gaming,4,7.4,8.0\n107,Christopher Watts,13,Male,South Josephstad,7th,5.6,5.8,85,4,0.6,1,2,8,1,1.4,44,7,2.1,0.0,1.5,Gaming,7,6.6,7.1\n108,Benjamin Bradley,15,Other,Rhondafurt,7th,7.3,5.0,67,2,0.5,8,5,5,0,1.4,77,18,1.2,1.1,0.8,Gaming,4,8.0,10.0\n109,Rebecca Edwards,18,Female,Jaredmouth,11th,4.6,9.4,55,2,0.8,2,4,5,1,1.1,113,7,1.6,1.7,1.6,Education,4,8.6,6.9\n110,Sarah Harrison,15,Other,North Cherylburgh,9th,2.0,5.9,74,5,0.9,2,9,6,0,1.1,47,12,2.0,1.3,1.4,Other,1,4.4,5.7\n111,Kimberly Hooper,14,Male,Bentleytown,9th,1.6,6.1,70,7,0.0,9,5,1,0,1.6,42,5,4.8,0.8,1.5,Social Media,3,1.8,6.0\n112,Melissa Becker DDS,18,Other,East Pamelaside,9th,2.0,7.5,76,0,1.1,1,7,2,1,0.9,115,7,1.4,1.9,1.3,Gaming,10,5.6,5.2\n113,John Griffin,16,Male,Loveberg,10th,3.0,4.7,51,10,3.0,1,8,1,1,0.3,42,18,1.7,3.3,0.0,Education,3,6.7,10.0\n114,James Alvarez,15,Female,Port Veronicamouth,9th,6.9,3.9,73,9,2.2,1,6,4,0,0.8,58,16,3.3,2.8,1.8,Education,3,4.9,10.0\n115,Brad Sanchez,16,Other,Port Samanthaberg,8th,5.5,5.4,63,8,0.0,5,10,3,1,2.0,72,14,1.4,0.0,0.8,Other,1,8.3,8.4\n116,Cory Santiago,15,Female,Robertshire,11th,6.6,9.5,53,5,1.0,8,4,1,0,0.8,121,13,4.5,1.7,2.8,Gaming,6,6.4,10.0\n117,Shelley Curtis,19,Other,South Brittanychester,12th,8.3,5.7,58,9,1.6,3,1,3,0,0.1,21,9,1.7,0.8,1.2,Other,10,6.5,10.0\n118,Cheryl Thomas,18,Other,East Chadhaven,8th,3.6,3.1,56,0,2.4,5,9,4,1,0.9,80,16,3.2,0.3,1.5,Social Media,7,7.3,10.0\n119,Meagan Banks,17,Male,Glennshire,9th,5.4,3.1,88,0,0.0,7,10,10,0,0.7,29,11,2.9,0.3,0.5,Social Media,3,1.7,9.8\n120,Dr. Elizabeth Porter,19,Other,Mirandaborough,9th,2.1,7.3,71,7,0.5,4,1,4,1,1.7,54,9,2.5,1.5,1.6,Education,5,2.1,5.3\n121,Monica Shelton,16,Male,Brownchester,7th,7.8,7.4,81,0,1.9,5,6,10,0,1.1,135,14,3.6,1.1,1.3,Browsing,6,4.5,10.0\n122,Heather Wilson,14,Male,Chelseafurt,9th,4.3,3.9,64,2,2.9,10,10,6,1,0.6,119,9,1.1,1.4,1.8,Other,4,6.7,9.0\n123,Angie White,18,Other,West Maryport,12th,7.4,7.4,80,0,1.1,1,2,3,0,1.9,57,20,1.9,0.9,1.6,Gaming,10,4.6,10.0\n124,Grace Mullen,18,Female,West Donna,7th,6.4,7.9,72,4,2.0,5,6,4,1,0.7,23,13,0.7,2.4,0.7,Other,3,7.4,8.6\n125,Megan Colon,13,Other,West Jeremy,9th,2.0,8.4,78,10,1.9,10,10,8,0,1.1,131,10,2.1,1.8,0.7,Social Media,8,3.9,10.0\n2988,Miss Emily Jenkins DDS,13,Other,South Theresa,12th,6.0,7.0,55,7,0.2,3,6,7,0,1.1,134,14,2.1,1.1,2.0,Other,7,5.0,10.0\n2989,Angela Lyons,14,Male,Howebury,7th,6.4,5.1,50,8,0.5,2,4,9,0,1.0,82,15,3.3,0.2,0.8,Education,10,7.5,10.0\n2990,Christine Davenport,18,Other,East Jeffreyburgh,10th,5.1,7.1,93,9,2.3,2,2,8,0,0.4,25,6,3.8,1.4,1.1,Browsing,6,6.8,8.4\n2991,Lauren Ballard,13,Female,East John,10th,2.8,5.6,92,4,1.1,7,10,7,1,1.1,77,10,3.7,3.4,0.7,Social Media,5,7.1,10.0\n2992,Christina Davila,16,Male,Heatherfurt,11th,6.3,5.7,87,8,2.9,6,1,8,0,0.8,70,14,3.5,0.8,1.4,Social Media,6,6.3,10.0\n\"\"\"\ngenerate_boxplots_all_numeric(file_content)","metadata":{"execution":{"iopub.execute_input":"2025-09-12T16:02:08.199526Z","iopub.status.busy":"2025-09-12T16:02:08.199022Z","iopub.status.idle":"2025-09-12T16:02:12.370013Z","shell.execute_reply":"2025-09-12T16:02:12.368620Z","shell.execute_reply.started":"2025-09-12T16:02:08.199503Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Re-import necessary libraries after code execution state reset\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Compute correlation matrix for numeric columns\ncorrelation_matrix = df.corr(numeric_only=True)\n\n# Plot the heatmap\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\nplt.title(\"Correlation Matrix of Teen Phone Addiction Dataset\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Features\")\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:45:05.876927Z","iopub.status.busy":"2025-09-12T15:45:05.875953Z","iopub.status.idle":"2025-09-12T15:45:07.036633Z","shell.execute_reply":"2025-09-12T15:45:07.035923Z","shell.execute_reply.started":"2025-09-12T15:45:05.876745Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# üìä MACHINE LEARNING PIPELINE: PHONE ADDICTION PREDICTION\n# =============================================================================\n\n# Import all required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Data processing and ML libraries\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\nfrom sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, \n                           mean_absolute_error, mean_squared_error, r2_score,\n                           precision_recall_fscore_support, roc_auc_score, roc_curve)\n\n# Regression Models\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\n\n# Classification Models  \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\n\n# Neural Networks\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nprint(\"‚úÖ All libraries imported successfully!\")\nprint(\"üöÄ Starting ML Pipeline for Phone Addiction Prediction...\")","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:45:07.038628Z","iopub.status.busy":"2025-09-12T15:45:07.037893Z","iopub.status.idle":"2025-09-12T15:45:15.522460Z","shell.execute_reply":"2025-09-12T15:45:15.521471Z","shell.execute_reply.started":"2025-09-12T15:45:07.038599Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# üìã DATA PREPARATION MODULE\n# =============================================================================\n\ndef prepare_data_for_ml():\n    \"\"\"\n    Prepare data for both regression and classification approaches\n    Returns: X, y_regression, y_classification, feature_names, label_encoders\n    \"\"\"\n    print(\"üîÑ Loading and preparing data...\")\n    \n    # Use the already loaded dataset\n    print(f\"üìä Original dataset shape: {df.shape}\")\n    \n    # Handle missing values\n    df_clean = df.dropna()\n    print(f\"üìä After removing missing values: {df_clean.shape}\")\n    \n    # Store original addiction level for regression\n    y_regression = df_clean['Addiction_Level'].copy()\n    \n    # Create classification target (binning continuous values)\n    df_clean['Addiction_Level_Category'] = pd.cut(df_clean['Addiction_Level'],\n        bins=[-np.inf, 3.5, 7.5, np.inf],\n        labels=['Low', 'Medium', 'High']\n    )\n    \n    # Drop original addiction level and rename categorical version\n    df_clean.drop(columns=['Addiction_Level'], inplace=True)\n    df_clean.rename(columns={'Addiction_Level_Category': 'Addiction_Level'}, inplace=True)\n    \n    # Check class distribution\n    print(\"\\nüìà Classification target distribution:\")\n    print(df_clean['Addiction_Level'].value_counts())\n    \n    # Encode categorical variables\n    label_encoders = {}\n    categorical_cols = df_clean.select_dtypes(include='object').columns.tolist()\n    \n    for col in categorical_cols:\n        le = LabelEncoder()\n        df_clean[col] = le.fit_transform(df_clean[col])\n        label_encoders[col] = le\n        print(f\"‚úÖ Encoded {col}: {len(le.classes_)} unique values\")\n    \n    # Prepare features and targets\n    X = df_clean.drop('Addiction_Level', axis=1)\n    y_classification = df_clean['Addiction_Level'].astype('category').cat.codes  # Convert to numeric codes\n    \n    # Store feature names\n    feature_names = X.columns.tolist()\n    \n    print(f\"\\n‚úÖ Data preparation complete!\")\n    print(f\"üìä Features shape: {X.shape}\")\n    print(f\"üìä Regression target shape: {y_regression.shape}\")\n    print(f\"üìä Classification target shape: {y_classification.shape}\")\n    print(f\"üìä Number of features: {len(feature_names)}\")\n    \n    return X, y_regression, y_classification, feature_names, label_encoders\n\n# Execute data preparation\nX, y_regression, y_classification, feature_names, label_encoders = prepare_data_for_ml()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# üîÑ TRAIN-TEST SPLIT & FEATURE SCALING\n# =============================================================================\n\ndef prepare_train_test_data(X, y_regression, y_classification, test_size=0.2, random_state=42):\n    \"\"\"\n    Split data into train/test sets and apply feature scaling\n    \"\"\"\n    print(\"üîÑ Splitting data into train/test sets...\")\n    \n    # Split for regression\n    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n        X, y_regression, test_size=test_size, random_state=random_state\n    )\n    \n    # Split for classification (with stratification)\n    X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n        X, y_classification, test_size=test_size, random_state=random_state, stratify=y_classification\n    )\n    \n    # Feature scaling\n    print(\"üîÑ Applying feature scaling...\")\n    scaler_reg = StandardScaler()\n    scaler_clf = StandardScaler()\n    \n    # Scale features for regression\n    X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n    X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n    \n    # Scale features for classification\n    X_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)\n    X_test_clf_scaled = scaler_clf.transform(X_test_clf)\n    \n    print(f\"‚úÖ Train/Test split complete!\")\n    print(f\"üìä Regression - Train: {X_train_reg_scaled.shape}, Test: {X_test_reg_scaled.shape}\")\n    print(f\"üìä Classification - Train: {X_train_clf_scaled.shape}, Test: {X_test_clf_scaled.shape}\")\n    \n    return {\n        'regression': {\n            'X_train': X_train_reg_scaled, 'X_test': X_test_reg_scaled,\n            'y_train': y_train_reg, 'y_test': y_test_reg,\n            'scaler': scaler_reg\n        },\n        'classification': {\n            'X_train': X_train_clf_scaled, 'X_test': X_test_clf_scaled,\n            'y_train': y_train_clf, 'y_test': y_test_clf,\n            'scaler': scaler_clf\n        }\n    }\n\n# Execute train-test split and scaling\ndata_splits = prepare_train_test_data(X, y_regression, y_classification)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# üìà REGRESSION APPROACH: PREDICTING CONTINUOUS ADDICTION LEVELS\n# =============================================================================\n\ndef train_regression_models(data_splits):\n    \"\"\"\n    Train and evaluate multiple regression models\n    \"\"\"\n    print(\"üöÄ Training Regression Models...\")\n    print(\"=\" * 60)\n    \n    # Define regression models\n    regression_models = {\n        'Linear Regression': LinearRegression(),\n        'Ridge Regression': Ridge(alpha=1.0),\n        'Lasso Regression': Lasso(alpha=0.1),\n        'Elastic Net': ElasticNet(alpha=0.1, l1_ratio=0.5),\n        'Decision Tree Regressor': DecisionTreeRegressor(random_state=42),\n        'Random Forest Regressor': RandomForestRegressor(n_estimators=100, random_state=42),\n        'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=42),\n        'XGBoost Regressor': XGBRegressor(random_state=42, eval_metric='rmse'),\n        'Support Vector Regressor': SVR(kernel='rbf', C=1.0, gamma='scale')\n    }\n    \n    # Get data splits\n    X_train = data_splits['regression']['X_train']\n    X_test = data_splits['regression']['X_test']\n    y_train = data_splits['regression']['y_train']\n    y_test = data_splits['regression']['y_test']\n    \n    # Store results\n    regression_results = {}\n    \n    for name, model in regression_models.items():\n        print(f\"\\nüîç Training {name}...\")\n        \n        try:\n            # Train model\n            model.fit(X_train, y_train)\n            \n            # Make predictions\n            y_pred_train = model.predict(X_train)\n            y_pred_test = model.predict(X_test)\n            \n            # Calculate metrics\n            train_mae = mean_absolute_error(y_train, y_pred_train)\n            test_mae = mean_absolute_error(y_test, y_pred_test)\n            train_mse = mean_squared_error(y_train, y_pred_train)\n            test_mse = mean_squared_error(y_test, y_pred_test)\n            train_rmse = np.sqrt(train_mse)\n            test_rmse = np.sqrt(test_mse)\n            train_r2 = r2_score(y_train, y_pred_train)\n            test_r2 = r2_score(y_test, y_pred_test)\n            \n            # Store results\n            regression_results[name] = {\n                'model': model,\n                'train_mae': train_mae,\n                'test_mae': test_mae,\n                'train_mse': train_mse,\n                'test_mse': test_mse,\n                'train_rmse': train_rmse,\n                'test_rmse': test_rmse,\n                'train_r2': train_r2,\n                'test_r2': test_r2,\n                'y_pred_test': y_pred_test\n            }\n            \n            print(f\"‚úÖ {name} - Test R¬≤: {test_r2:.4f}, Test RMSE: {test_rmse:.4f}\")\n            \n        except Exception as e:\n            print(f\"‚ùå Error training {name}: {str(e)}\")\n            continue\n    \n    return regression_results\n\n# Train regression models\nregression_results = train_regression_models(data_splits)\n","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:45:15.525755Z","iopub.status.busy":"2025-09-12T15:45:15.525483Z","iopub.status.idle":"2025-09-12T15:45:18.539732Z","shell.execute_reply":"2025-09-12T15:45:18.538786Z","shell.execute_reply.started":"2025-09-12T15:45:15.525735Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# üìä REGRESSION RESULTS VISUALIZATION\n# =============================================================================\n\ndef visualize_regression_results(regression_results, data_splits):\n    \"\"\"\n    Create comprehensive visualizations for regression results\n    \"\"\"\n    print(\"üìä Creating regression results visualizations...\")\n    \n    # Create results DataFrame\n    results_data = []\n    for name, results in regression_results.items():\n        results_data.append({\n            'Model': name,\n            'Test R¬≤': results['test_r2'],\n            'Test RMSE': results['test_rmse'],\n            'Test MAE': results['test_mae'],\n            'Train R¬≤': results['train_r2'],\n            'Train RMSE': results['train_rmse']\n        })\n    \n    results_df = pd.DataFrame(results_data).sort_values('Test R¬≤', ascending=False)\n    \n    # 1. Model Performance Comparison\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # R¬≤ Score comparison\n    sns.barplot(data=results_df, x='Test R¬≤', y='Model', ax=axes[0,0], palette='viridis')\n    axes[0,0].set_title('Model Performance: R¬≤ Score', fontsize=14, fontweight='bold')\n    axes[0,0].set_xlabel('R¬≤ Score')\n    axes[0,0].grid(True, alpha=0.3)\n    \n    # RMSE comparison\n    sns.barplot(data=results_df, x='Test RMSE', y='Model', ax=axes[0,1], palette='plasma')\n    axes[0,1].set_title('Model Performance: RMSE', fontsize=14, fontweight='bold')\n    axes[0,1].set_xlabel('RMSE')\n    axes[0,1].grid(True, alpha=0.3)\n    \n    # MAE comparison\n    sns.barplot(data=results_df, x='Test MAE', y='Model', ax=axes[1,0], palette='coolwarm')\n    axes[1,0].set_title('Model Performance: MAE', fontsize=14, fontweight='bold')\n    axes[1,0].set_xlabel('MAE')\n    axes[1,0].grid(True, alpha=0.3)\n    \n    # Train vs Test R¬≤ comparison\n    x_pos = np.arange(len(results_df))\n    width = 0.35\n    axes[1,1].bar(x_pos - width/2, results_df['Train R¬≤'], width, label='Train R¬≤', alpha=0.8)\n    axes[1,1].bar(x_pos + width/2, results_df['Test R¬≤'], width, label='Test R¬≤', alpha=0.8)\n    axes[1,1].set_xlabel('Models')\n    axes[1,1].set_ylabel('R¬≤ Score')\n    axes[1,1].set_title('Train vs Test R¬≤ Comparison', fontsize=14, fontweight='bold')\n    axes[1,1].set_xticks(x_pos)\n    axes[1,1].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n    axes[1,1].legend()\n    axes[1,1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 2. Actual vs Predicted plots for top 3 models\n    top_3_models = results_df.head(3)['Model'].tolist()\n    y_test = data_splits['regression']['y_test']\n    \n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n    \n    for i, model_name in enumerate(top_3_models):\n        y_pred = regression_results[model_name]['y_pred_test']\n        \n        axes[i].scatter(y_test, y_pred, alpha=0.6, s=20)\n        axes[i].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n        axes[i].set_xlabel('Actual Addiction Level')\n        axes[i].set_ylabel('Predicted Addiction Level')\n        axes[i].set_title(f'{model_name}\\nR¬≤ = {regression_results[model_name][\"test_r2\"]:.4f}')\n        axes[i].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 3. Print detailed results table\n    print(\"\\nüìà Detailed Regression Results:\")\n    print(\"=\" * 80)\n    print(results_df.round(4).to_string(index=False))\n    \n    return results_df\n\n# Visualize regression results\nregression_summary = visualize_regression_results(regression_results, data_splits)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# üéØ CLASSIFICATION APPROACH: PREDICTING ADDICTION CATEGORIES\n# =============================================================================\n\ndef train_classification_models(data_splits):\n    \"\"\"\n    Train and evaluate multiple classification models\n    \"\"\"\n    print(\"üöÄ Training Classification Models...\")\n    print(\"=\" * 60)\n    \n    # Define classification models\n    classification_models = {\n        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n        'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n        'Decision Tree Classifier': DecisionTreeClassifier(random_state=42),\n        'Random Forest Classifier': RandomForestClassifier(n_estimators=100, random_state=42),\n        'Gradient Boosting Classifier': GradientBoostingClassifier(random_state=42),\n        'XGBoost Classifier': XGBClassifier(random_state=42, eval_metric='mlogloss'),\n        'Support Vector Classifier': SVC(probability=True, random_state=42),\n        'Neural Network (MLP)': MLPClassifier(max_iter=500, random_state=42)\n    }\n    \n    # Get data splits\n    X_train = data_splits['classification']['X_train']\n    X_test = data_splits['classification']['X_test']\n    y_train = data_splits['classification']['y_train']\n    y_test = data_splits['classification']['y_test']\n    \n    # Store results\n    classification_results = {}\n    \n    for name, model in classification_models.items():\n        print(f\"\\nüîç Training {name}...\")\n        \n        try:\n            # Train model\n            model.fit(X_train, y_train)\n            \n            # Make predictions\n            y_pred_train = model.predict(X_train)\n            y_pred_test = model.predict(X_test)\n            \n            # Calculate metrics\n            train_acc = accuracy_score(y_train, y_pred_train)\n            test_acc = accuracy_score(y_test, y_pred_test)\n            \n            # Get detailed classification report\n            train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(\n                y_train, y_pred_train, average='weighted', zero_division=0\n            )\n            test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(\n                y_test, y_pred_test, average='weighted', zero_division=0\n            )\n            \n            # Store results\n            classification_results[name] = {\n                'model': model,\n                'train_accuracy': train_acc,\n                'test_accuracy': test_acc,\n                'train_precision': train_precision,\n                'test_precision': test_precision,\n                'train_recall': train_recall,\n                'test_recall': test_recall,\n                'train_f1': train_f1,\n                'test_f1': test_f1,\n                'y_pred_test': y_pred_test,\n                'y_pred_proba': model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n            }\n            \n            print(f\"‚úÖ {name} - Test Accuracy: {test_acc:.4f}, Test F1: {test_f1:.4f}\")\n            \n        except Exception as e:\n            print(f\"‚ùå Error training {name}: {str(e)}\")\n            continue\n    \n    return classification_results\n\n# Train classification models\nclassification_results = train_classification_models(data_splits)\n","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:45:18.541126Z","iopub.status.busy":"2025-09-12T15:45:18.540815Z","iopub.status.idle":"2025-09-12T15:45:20.431596Z","shell.execute_reply":"2025-09-12T15:45:20.430788Z","shell.execute_reply.started":"2025-09-12T15:45:18.541105Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# üìä CLASSIFICATION RESULTS VISUALIZATION\n# =============================================================================\n\ndef visualize_classification_results(classification_results, data_splits):\n    \"\"\"\n    Create comprehensive visualizations for classification results\n    \"\"\"\n    print(\"üìä Creating classification results visualizations...\")\n    \n    # Create results DataFrame\n    results_data = []\n    for name, results in classification_results.items():\n        results_data.append({\n            'Model': name,\n            'Test Accuracy': results['test_accuracy'],\n            'Test Precision': results['test_precision'],\n            'Test Recall': results['test_recall'],\n            'Test F1-Score': results['test_f1'],\n            'Train Accuracy': results['train_accuracy']\n        })\n    \n    results_df = pd.DataFrame(results_data).sort_values('Test Accuracy', ascending=False)\n    \n    # 1. Model Performance Comparison\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Accuracy comparison\n    sns.barplot(data=results_df, x='Test Accuracy', y='Model', ax=axes[0,0], palette='viridis')\n    axes[0,0].set_title('Model Performance: Accuracy', fontsize=14, fontweight='bold')\n    axes[0,0].set_xlabel('Accuracy')\n    axes[0,0].grid(True, alpha=0.3)\n    \n    # F1-Score comparison\n    sns.barplot(data=results_df, x='Test F1-Score', y='Model', ax=axes[0,1], palette='plasma')\n    axes[0,1].set_title('Model Performance: F1-Score', fontsize=14, fontweight='bold')\n    axes[0,1].set_xlabel('F1-Score')\n    axes[0,1].grid(True, alpha=0.3)\n    \n    # Precision comparison\n    sns.barplot(data=results_df, x='Test Precision', y='Model', ax=axes[1,0], palette='coolwarm')\n    axes[1,0].set_title('Model Performance: Precision', fontsize=14, fontweight='bold')\n    axes[1,0].set_xlabel('Precision')\n    axes[1,0].grid(True, alpha=0.3)\n    \n    # Train vs Test Accuracy comparison\n    x_pos = np.arange(len(results_df))\n    width = 0.35\n    axes[1,1].bar(x_pos - width/2, results_df['Train Accuracy'], width, label='Train Accuracy', alpha=0.8)\n    axes[1,1].bar(x_pos + width/2, results_df['Test Accuracy'], width, label='Test Accuracy', alpha=0.8)\n    axes[1,1].set_xlabel('Models')\n    axes[1,1].set_ylabel('Accuracy')\n    axes[1,1].set_title('Train vs Test Accuracy Comparison', fontsize=14, fontweight='bold')\n    axes[1,1].set_xticks(x_pos)\n    axes[1,1].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n    axes[1,1].legend()\n    axes[1,1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 2. Confusion Matrices for top 3 models\n    top_3_models = results_df.head(3)['Model'].tolist()\n    y_test = data_splits['classification']['y_test']\n    \n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n    \n    for i, model_name in enumerate(top_3_models):\n        y_pred = classification_results[model_name]['y_pred_test']\n        cm = confusion_matrix(y_test, y_pred)\n        \n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n        axes[i].set_title(f'{model_name}\\nAccuracy: {classification_results[model_name][\"test_accuracy\"]:.4f}')\n        axes[i].set_xlabel('Predicted')\n        axes[i].set_ylabel('Actual')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 3. Print detailed results table\n    print(\"\\nüìà Detailed Classification Results:\")\n    print(\"=\" * 80)\n    print(results_df.round(4).to_string(index=False))\n    \n    return results_df\n\n# Visualize classification results\nclassification_summary = visualize_classification_results(classification_results, data_splits)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# üîç COMPREHENSIVE MODEL COMPARISON & INSIGHTS\n# =============================================================================\n\ndef comprehensive_model_comparison(regression_results, classification_results, feature_names):\n    \"\"\"\n    Create comprehensive comparison between regression and classification approaches\n    \"\"\"\n    print(\"üîç Creating comprehensive model comparison...\")\n    \n    # 1. Best Model Selection\n    best_regression = max(regression_results.items(), key=lambda x: x[1]['test_r2'])\n    best_classification = max(classification_results.items(), key=lambda x: x[1]['test_accuracy'])\n    \n    print(f\"\\nüèÜ BEST MODELS:\")\n    print(f\"üìà Best Regression Model: {best_regression[0]} (R¬≤ = {best_regression[1]['test_r2']:.4f})\")\n    print(f\"üéØ Best Classification Model: {best_classification[0]} (Accuracy = {best_classification[1]['test_accuracy']:.4f})\")\n    \n    # 2. Feature Importance Analysis\n    print(f\"\\nüîç FEATURE IMPORTANCE ANALYSIS:\")\n    \n    # Get feature importance from best models\n    if hasattr(best_regression[1]['model'], 'feature_importances_'):\n        reg_importance = best_regression[1]['model'].feature_importances_\n        reg_features = pd.DataFrame({\n            'Feature': feature_names,\n            'Importance': reg_importance\n        }).sort_values('Importance', ascending=False)\n        \n        print(f\"\\nüìà Top 10 Features (Regression - {best_regression[0]}):\")\n        print(reg_features.head(10).to_string(index=False))\n    \n    if hasattr(best_classification[1]['model'], 'feature_importances_'):\n        clf_importance = best_classification[1]['model'].feature_importances_\n        clf_features = pd.DataFrame({\n            'Feature': feature_names,\n            'Importance': clf_importance\n        }).sort_values('Importance', ascending=False)\n        \n        print(f\"\\nüéØ Top 10 Features (Classification - {best_classification[0]}):\")\n        print(clf_features.head(10).to_string(index=False))\n    \n    # 3. Performance Summary Comparison\n    print(f\"\\nüìä PERFORMANCE SUMMARY:\")\n    print(\"=\" * 80)\n    \n    # Regression summary\n    reg_summary = pd.DataFrame([\n        {\n            'Approach': 'Regression',\n            'Best Model': best_regression[0],\n            'R¬≤ Score': best_regression[1]['test_r2'],\n            'RMSE': best_regression[1]['test_rmse'],\n            'MAE': best_regression[1]['test_mae']\n        }\n    ])\n    \n    # Classification summary\n    clf_summary = pd.DataFrame([\n        {\n            'Approach': 'Classification',\n            'Best Model': best_classification[0],\n            'Accuracy': best_classification[1]['test_accuracy'],\n            'F1-Score': best_classification[1]['test_f1'],\n            'Precision': best_classification[1]['test_precision']\n        }\n    ])\n    \n    print(\"Regression Results:\")\n    print(reg_summary.round(4).to_string(index=False))\n    print(\"\\nClassification Results:\")\n    print(clf_summary.round(4).to_string(index=False))\n    \n    # 4. Visual Comparison\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    \n    # Regression vs Classification performance comparison\n    approaches = ['Regression', 'Classification']\n    r2_scores = [best_regression[1]['test_r2'], best_classification[1]['test_accuracy']]\n    \n    axes[0,0].bar(approaches, r2_scores, color=['skyblue', 'lightcoral'], alpha=0.7)\n    axes[0,0].set_title('Best Model Performance Comparison', fontsize=14, fontweight='bold')\n    axes[0,0].set_ylabel('Score (R¬≤ for Regression, Accuracy for Classification)')\n    axes[0,0].grid(True, alpha=0.3)\n    \n    # Add value labels on bars\n    for i, v in enumerate(r2_scores):\n        axes[0,0].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom', fontweight='bold')\n    \n    # Feature importance comparison (if available)\n    if hasattr(best_regression[1]['model'], 'feature_importances_') and hasattr(best_classification[1]['model'], 'feature_importances_'):\n        top_n = 10\n        reg_top = reg_features.head(top_n)\n        clf_top = clf_features.head(top_n)\n        \n        # Plot top features for regression\n        axes[0,1].barh(range(len(reg_top)), reg_top['Importance'], color='skyblue', alpha=0.7)\n        axes[0,1].set_yticks(range(len(reg_top)))\n        axes[0,1].set_yticklabels(reg_top['Feature'], fontsize=8)\n        axes[0,1].set_title(f'Top {top_n} Features - Regression', fontsize=12, fontweight='bold')\n        axes[0,1].set_xlabel('Importance')\n        \n        # Plot top features for classification\n        axes[1,0].barh(range(len(clf_top)), clf_top['Importance'], color='lightcoral', alpha=0.7)\n        axes[1,0].set_yticks(range(len(clf_top)))\n        axes[1,0].set_yticklabels(clf_top['Feature'], fontsize=8)\n        axes[1,0].set_title(f'Top {top_n} Features - Classification', fontsize=12, fontweight='bold')\n        axes[1,0].set_xlabel('Importance')\n    \n    # Model complexity vs performance\n    reg_models = list(regression_results.keys())\n    reg_r2_scores = [regression_results[model]['test_r2'] for model in reg_models]\n    \n    clf_models = list(classification_results.keys())\n    clf_acc_scores = [classification_results[model]['test_accuracy'] for model in clf_models]\n    \n    axes[1,1].scatter(range(len(reg_models)), reg_r2_scores, label='Regression (R¬≤)', alpha=0.7, s=60)\n    axes[1,1].scatter(range(len(clf_models)), clf_acc_scores, label='Classification (Accuracy)', alpha=0.7, s=60)\n    axes[1,1].set_title('All Models Performance Distribution', fontsize=12, fontweight='bold')\n    axes[1,1].set_xlabel('Model Index')\n    axes[1,1].set_ylabel('Performance Score')\n    axes[1,1].legend()\n    axes[1,1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return best_regression, best_classification\n\n# Execute comprehensive comparison\nbest_reg, best_clf = comprehensive_model_comparison(regression_results, classification_results, feature_names)","metadata":{"execution":{"iopub.execute_input":"2025-09-12T15:45:20.432933Z","iopub.status.busy":"2025-09-12T15:45:20.432613Z","iopub.status.idle":"2025-09-12T15:45:57.558897Z","shell.execute_reply":"2025-09-12T15:45:57.557943Z","shell.execute_reply.started":"2025-09-12T15:45:20.432907Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# üéØ FINAL CONCLUSIONS & RECOMMENDATIONS\n# =============================================================================\n\ndef generate_final_conclusions(best_reg, best_clf, regression_results, classification_results):\n    \"\"\"\n    Generate comprehensive conclusions and recommendations\n    \"\"\"\n    print(\"üéØ FINAL CONCLUSIONS & RECOMMENDATIONS\")\n    print(\"=\" * 80)\n    \n    # Performance Summary\n    print(f\"\\nüìä PERFORMANCE SUMMARY:\")\n    print(f\"üèÜ Best Regression Model: {best_reg[0]}\")\n    print(f\"   ‚Ä¢ R¬≤ Score: {best_reg[1]['test_r2']:.4f}\")\n    print(f\"   ‚Ä¢ RMSE: {best_reg[1]['test_rmse']:.4f}\")\n    print(f\"   ‚Ä¢ MAE: {best_reg[1]['test_mae']:.4f}\")\n    \n    print(f\"\\nüèÜ Best Classification Model: {best_clf[0]}\")\n    print(f\"   ‚Ä¢ Accuracy: {best_clf[1]['test_accuracy']:.4f}\")\n    print(f\"   ‚Ä¢ F1-Score: {best_clf[1]['test_f1']:.4f}\")\n    print(f\"   ‚Ä¢ Precision: {best_clf[1]['test_precision']:.4f}\")\n    \n    # Approach Comparison\n    print(f\"\\nüîç REGRESSION vs CLASSIFICATION COMPARISON:\")\n    if best_reg[1]['test_r2'] > 0.8:\n        print(\"‚úÖ Regression approach shows excellent performance (R¬≤ > 0.8)\")\n    elif best_reg[1]['test_r2'] > 0.6:\n        print(\"‚ö†Ô∏è Regression approach shows good performance (R¬≤ > 0.6)\")\n    else:\n        print(\"‚ùå Regression approach shows moderate performance (R¬≤ < 0.6)\")\n    \n    if best_clf[1]['test_accuracy'] > 0.9:\n        print(\"‚úÖ Classification approach shows excellent performance (Accuracy > 0.9)\")\n    elif best_clf[1]['test_accuracy'] > 0.8:\n        print(\"‚ö†Ô∏è Classification approach shows good performance (Accuracy > 0.8)\")\n    else:\n        print(\"‚ùå Classification approach shows moderate performance (Accuracy < 0.8)\")\n    \n    # Model Recommendations\n    print(f\"\\nüí° MODEL RECOMMENDATIONS:\")\n    print(f\"üéØ For Production Use: {best_clf[0] if best_clf[1]['test_accuracy'] > best_reg[1]['test_r2'] else best_reg[0]}\")\n    print(f\"üìà For Research: Both approaches provide valuable insights\")\n    \n    # Real-World Applications\n    print(f\"\\nüåç REAL-WORLD APPLICATIONS:\")\n    print(\"üö® Early Intervention: Identify at-risk teens for phone addiction\")\n    print(\"üì± Personalized Support: Develop targeted intervention programs\")\n    print(\"üè• Healthcare: Support mental health assessments\")\n    print(\"üìö Education: Inform school policies on digital wellness\")\n    print(\"üî¨ Research: Contribute to addiction studies\")\n    \n    # Future Improvements\n    print(f\"\\nüöÄ FUTURE IMPROVEMENTS:\")\n    print(\"‚öôÔ∏è Hyperparameter Tuning: Optimize model parameters\")\n    print(\"üìä Feature Engineering: Create new predictive features\")\n    print(\"üîÑ Cross-Validation: Implement robust validation strategies\")\n    print(\"üìà Ensemble Methods: Combine multiple models\")\n    print(\"üéØ Deep Learning: Explore advanced neural architectures\")\n    print(\"üì± Real-time Data: Incorporate time-series features\")\n    \n    # Technical Insights\n    print(f\"\\nüî¨ TECHNICAL INSIGHTS:\")\n    print(f\"‚Ä¢ Dataset Size: {len(data_splits['regression']['X_train']) + len(data_splits['regression']['X_test'])} samples\")\n    print(f\"‚Ä¢ Features: {len(feature_names)} features\")\n    print(f\"‚Ä¢ Models Tested: {len(regression_results)} regression + {len(classification_results)} classification\")\n    print(f\"‚Ä¢ Best Performance: {max(best_reg[1]['test_r2'], best_clf[1]['test_accuracy']):.4f}\")\n    \n    print(f\"\\n‚ú® This comprehensive analysis provides a solid foundation for\")\n    print(f\"   phone addiction prediction and intervention strategies!\")\n\n# Generate final conclusions\ngenerate_final_conclusions(best_reg, best_clf, regression_results, classification_results)","metadata":{"execution":{"iopub.execute_input":"2025-09-12T16:17:06.038519Z","iopub.status.busy":"2025-09-12T16:17:06.037528Z","iopub.status.idle":"2025-09-12T16:17:06.044196Z","shell.execute_reply":"2025-09-12T16:17:06.043243Z","shell.execute_reply.started":"2025-09-12T16:17:06.038486Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"If you have come so far,an upvote would be appreciated!","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}